{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9Ry9Pa7dq_dZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGESH.N.A\n",
        "212223240078"
      ],
      "metadata": {
        "id": "fuVDVaE9IpkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "XmAVeucbrcMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6389e12-1452-49d1-a2e5-f341035f35f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare data\n",
        "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\").ffill()\n",
        "words = list(data[\"Word\"].unique())\n",
        "tags = list(data[\"Tag\"].unique())\n",
        "\n",
        "if \"ENDPAD\" not in words:\n",
        "    words.append(\"ENDPAD\")\n",
        "\n",
        "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n",
        "idx2tag = {i: t for t, i in tag2idx.items()}"
      ],
      "metadata": {
        "id": "bE8VEjpnrc8C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "d72a60a8-e01d-4536-b91f-549cbc99d6ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'ner_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0b7f26aa39fc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner_dataset.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Word\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tag\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ner_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(50)"
      ],
      "metadata": {
        "id": "2DGpH3WzrmDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essential info about tagged entities:\n",
        "```\n",
        "geo = Geographical Entity\n",
        "org = Organization\n",
        "per = Person\n",
        "gpe = Geopolitical Entity\n",
        "tim = Time indicator\n",
        "art = Artifact\n",
        "eve = Event\n",
        "nat = Natural Phenomenon\n",
        "```"
      ],
      "metadata": {
        "id": "b-xy1yJtIUdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique words in corpus:\", data['Word'].nunique())\n",
        "print(\"Unique tags in corpus:\", data['Tag'].nunique())"
      ],
      "metadata": {
        "id": "aOWkBoLXsPvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique tags are:\", tags)"
      ],
      "metadata": {
        "id": "m6_c7uwHsa_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group words by sentences\n",
        "class SentenceGetter:\n",
        "    def __init__(self, data):\n",
        "        self.grouped = data.groupby(\"Sentence #\", group_keys=False).apply(\n",
        "            lambda s: [(w, t) for w, t in zip(s[\"Word\"], s[\"Tag\"])]\n",
        "        )\n",
        "        self.sentences = list(self.grouped)\n",
        "\n",
        "getter = SentenceGetter(data)\n",
        "sentences = getter.sentences\n"
      ],
      "metadata": {
        "id": "-Zo6uyoCsiEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[35]"
      ],
      "metadata": {
        "id": "Z-pZFuuAs4Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode sentences\n",
        "X = [[word2idx[w] for w, t in s] for s in sentences]\n",
        "y = [[tag2idx[t] for w, t in s] for s in sentences]"
      ],
      "metadata": {
        "id": "-3Yd8hqMssdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ipTK2DOstD3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGESH.N.A\n",
        "212223240078"
      ],
      "metadata": {
        "id": "qWgY2fqKIrl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eWV-M4IftKUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences\n",
        "max_len = 50\n",
        "X_pad = pad_sequence([torch.tensor(seq) for seq in X], batch_first=True, padding_value=word2idx[\"ENDPAD\"])\n",
        "y_pad = pad_sequence([torch.tensor(seq) for seq in y], batch_first=True, padding_value=tag2idx[\"O\"])\n",
        "X_pad = X_pad[:, :max_len]\n",
        "y_pad = y_pad[:, :max_len]"
      ],
      "metadata": {
        "id": "m6hFevHstXKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pad[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TA2On7iAtzAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pad[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8Cx7lJhft4aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_pad, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "74HBxw81tap6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.X[idx],\n",
        "            \"labels\": self.y[idx]\n",
        "        }\n",
        "\n",
        "train_loader = DataLoader(NERDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(NERDataset(X_test, y_test), batch_size=32)\n"
      ],
      "metadata": {
        "id": "ukXTrKSPtipK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "# Load and prepare data\n",
        "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\").ffill()\n",
        "words = list(data[\"Word\"].unique())\n",
        "tags = list(data[\"Tag\"].unique())\n",
        "\n",
        "if \"ENDPAD\" not in words:\n",
        "    words.append(\"ENDPAD\")\n",
        "\n",
        "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n",
        "idx2tag = {i: t for t, i in tag2idx.items()}\n",
        "data.head(50)\n",
        "print(\"Unique words in corpus:\", data['Word'].nunique())\n",
        "print(\"Unique tags in corpus:\", data['Tag'].nunique())\n",
        "print(\"Unique tags are:\", tags)\n",
        "# Group words by sentences\n",
        "class SentenceGetter:\n",
        "    def __init__(self, data):\n",
        "        self.grouped = data.groupby(\"Sentence #\", group_keys=False).apply(\n",
        "            lambda s: [(w, t) for w, t in zip(s[\"Word\"], s[\"Tag\"])]\n",
        "        )\n",
        "        self.sentences = list(self.grouped)\n",
        "\n",
        "getter = SentenceGetter(data)\n",
        "sentences = getter.sentences\n",
        "sentences[35]\n",
        "# Encode sentences\n",
        "X = [[word2idx[w] for w, t in s] for s in sentences]\n",
        "y = [[tag2idx[t] for w, t in s] for s in sentences]\n",
        "word2idx\n",
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.show()\n",
        "# Pad sequences\n",
        "max_len = 50\n",
        "X_pad = pad_sequence([torch.tensor(seq) for seq in X], batch_first=True, padding_value=word2idx[\"ENDPAD\"])\n",
        "y_pad = pad_sequence([torch.tensor(seq) for seq in y], batch_first=True, padding_value=tag2idx[\"O\"])\n",
        "X_pad = X_pad[:, :max_len]\n",
        "y_pad = y_pad[:, :max_len]\n",
        "X_pad[0]\n",
        "y_pad[0]\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_pad, test_size=0.2, random_state=1)\n",
        "\n",
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.X[idx],\n",
        "            \"labels\": self.y[idx]\n",
        "        }\n",
        "\n",
        "train_loader = DataLoader(NERDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(NERDataset(X_test, y_test), batch_size=32)\n"
      ],
      "metadata": {
        "id": "q6GMDf-6piJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    # Include your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Include your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MFu8201Qtnl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =\n",
        "loss_fn =\n",
        "optimizer ="
      ],
      "metadata": {
        "id": "onjJJSuAuFyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Evaluation Functions\n",
        "def train_model(model, train_loader, test_loader, loss_fn, optimizer, epochs=3):\n",
        "    # Include the training and evaluation functions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "xlcDlPEruRB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, X_test, y_test):\n",
        "    model.eval()\n",
        "    true_tags, pred_tags = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "            for i in range(len(labels)):\n",
        "                for j in range(len(labels[i])):\n",
        "                    if labels[i][j] != tag2idx[\"O\"]:\n",
        "                        true_tags.append(idx2tag[labels[i][j].item()])\n",
        "                        pred_tags.append(idx2tag[preds[i][j].item()])\n"
      ],
      "metadata": {
        "id": "rALxE4gTuVUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training and evaluation\n",
        "train_losses, val_losses = train_model(model, train_loader, test_loader, loss_fn, optimizer, epochs=3)\n",
        "evaluate_model(model, test_loader, X_test, y_test)"
      ],
      "metadata": {
        "id": "AVJ5f5hyubC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss\n",
        "print('Name:                 ')\n",
        "print('Register Number:     ')\n",
        "history_df = pd.DataFrame({\"loss\": train_losses, \"val_loss\": val_losses})\n",
        "history_df.plot(title=\"Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8HpgHVLhwPWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference and prediction\n",
        "i = 125\n",
        "model.eval()\n",
        "sample = X_test[i].unsqueeze(0).to(device)\n",
        "output = model(sample)\n",
        "preds = torch.argmax(output, dim=-1).squeeze().cpu().numpy()\n",
        "true = y_test[i].numpy()\n",
        "\n",
        "print('Name:                 ')\n",
        "print('Register Number:     ')\n",
        "print(\"{:<15} {:<10} {}\\n{}\".format(\"Word\", \"True\", \"Pred\", \"-\" * 40))\n",
        "for w_id, true_tag, pred_tag in zip(X_test[i], y_test[i], preds):\n",
        "    if w_id.item() != word2idx[\"ENDPAD\"]:\n",
        "        word = words[w_id.item() - 1]\n",
        "        true_label = tags[true_tag.item()]\n",
        "        pred_label = tags[pred_tag]\n",
        "        print(f\"{word:<15} {true_label:<10} {pred_label}\")"
      ],
      "metadata": {
        "id": "C7YGTaIPwQEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGESH.N.A\n",
        "212223240078"
      ],
      "metadata": {
        "id": "0JLKHrK-3C6h"
      }
    }
  ]
}